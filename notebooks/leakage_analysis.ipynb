{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating model generations in terms of semantic leakage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installations and imports necessary to run the code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ! pip3 install evaluate\n",
    "# ! pip install bert_score\n",
    "# ! pip install -U ipywidgets\n",
    "# ! pip install openpyxl\n",
    "# ! pip install sentence_transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "import pandas as pd\n",
    "from evaluate import load\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm as tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading backbone models to calculate semantic similarity for Leak-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/veronicasmilga/Desktop/TuÌˆbingen/UnderLLMs/semantic_leakage_project/.venv/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "bertscore = load(\"bertscore\")\n",
    "st_minilm = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions to calculate Leak-Rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_bert_score(concept, generation):\n",
    "    \"\"\"\n",
    "    Compute the BERTScore similarity between a concept and a generation.\n",
    "\n",
    "    Args:\n",
    "        concept (str): The reference concept.\n",
    "        generation (str): The generated text to compare against the concept.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score (F1) between the concept and the generation.\n",
    "    \"\"\"\n",
    "    result = bertscore.compute(predictions=[concept], references=[generation], model_type=\"distilbert-base-uncased\")\n",
    "    similarity = result[\"f1\"][0]\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_st_minilm_score(concept, generation):\n",
    "    \"\"\"\n",
    "    Compute the similarity between a concept and a generation using SBERT MiniLM embeddings.\n",
    "\n",
    "    Args:\n",
    "        concept (str): The reference concept.\n",
    "        generation (str): The generated text to compare against the concept.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score between the concept and the generation.\n",
    "    \"\"\"\n",
    "    embedding_concept = st_minilm.encode(concept)\n",
    "    embedding_generation = st_minilm.encode(generation)\n",
    "    similarity = st_minilm.similarity(embedding_concept, embedding_generation)\n",
    "    similarity = float(similarity[0][0])\n",
    "    return similarity\n",
    "\n",
    "\n",
    "def get_leak_rate_for_pair(concept, control_generation, test_generation, score=\"bertscore\"):\n",
    "    \"\"\"\n",
    "    Determine the leak rate for a single pair of control and test generations compared to a concept.\n",
    "\n",
    "    Args:\n",
    "        concept (str): The reference concept.\n",
    "        control_generation (str): The control generation.\n",
    "        test_generation (str): The test generation.\n",
    "        score (str, optional): The scoring method to use ('bertscore' or 'sb_minilm'). Defaults to 'bertscore'.\n",
    "\n",
    "    Returns:\n",
    "        float: The leak rate for the pair, where 1 indicates leakage, 0.5 indicates neutrality, and 0 indicates no leakage.\n",
    "    \"\"\"\n",
    "    if score == \"bertscore\":\n",
    "        score_control = round(get_bert_score(concept, control_generation), 3)\n",
    "        score_test = round(get_bert_score(concept, test_generation), 3)\n",
    "    elif score == \"sb_minilm\":\n",
    "        score_control = round(get_st_minilm_score(concept, control_generation), 3)\n",
    "        score_test = round(get_st_minilm_score(concept, test_generation), 3)\n",
    "\n",
    "    if score_test > score_control:\n",
    "        leak_rate = 1\n",
    "    elif score_test == score_control:\n",
    "        leak_rate = 0.5\n",
    "    else:\n",
    "        leak_rate = 0\n",
    "\n",
    "    return leak_rate\n",
    "\n",
    "\n",
    "def get_leak_rate_for_df(df, n_generations=1, score=\"bertscore\", cat=None):\n",
    "    \"\"\"\n",
    "    Calculate the average leak rate for a dataframe containing concepts and generations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The dataframe containing concepts, control generations, and test generations.\n",
    "        n_generations (int, optional): The number of generations to evaluate for each concept. Defaults to 1.\n",
    "        score (str, optional): The scoring method to use ('bertscore' or 'sb_minilm'). Defaults to 'bertscore'.\n",
    "        cat (str, optional): A specific category to filter the rows by. If None, all rows are considered. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        float: The average leak rate across all evaluated pairs in the dataframe.\n",
    "    \"\"\"\n",
    "    bert_scores = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "\n",
    "        if cat and row[\"category\"] != cat:\n",
    "            continue\n",
    "\n",
    "        concept = row[\"concept\"]\n",
    "        n_control_row = row[\"n_control\"]\n",
    "\n",
    "        if n_control_row is None or n_control_row == \"none\" or (isinstance(n_control_row, float) and np.isnan(n_control_row)):\n",
    "            continue\n",
    "\n",
    "        control_row = df.iloc[int(n_control_row)]\n",
    "\n",
    "        for n_gen in range(n_generations):\n",
    "            test_generation = row[f\"generation_{n_gen+1}\"]\n",
    "            control_generation = control_row[f\"generation_{n_gen+1}\"]\n",
    "            leak_rate_for_pair = get_leak_rate_for_pair(concept, control_generation, test_generation, score=score)\n",
    "            bert_scores.append(leak_rate_for_pair)\n",
    "\n",
    "    leak_rate = np.mean(bert_scores)\n",
    "    return leak_rate\n",
    "\n",
    "\n",
    "def get_leak_rates_for_all_model_sizes(filenames, n_generations=1):\n",
    "    \"\"\"\n",
    "    Calculate leak rates for multiple model sizes using BERTScore and SBERT MiniLM.\n",
    "\n",
    "    Args:\n",
    "        filenames (list of str): List of file paths to the data files.\n",
    "        n_generations (int, optional): Number of generations to evaluate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing leak rates for each file and scoring method.\n",
    "    \"\"\"\n",
    "    scores = {}\n",
    "\n",
    "    for filename in filenames:\n",
    "        scores[filename] = {}\n",
    "        df = pd.read_excel(filename)\n",
    "        bs = get_leak_rate_for_df(\n",
    "            df,\n",
    "            n_generations=n_generations,\n",
    "            score=\"bertscore\"\n",
    "        )\n",
    "        sb_minilm = get_leak_rate_for_df(\n",
    "            df,\n",
    "            n_generations=n_generations,\n",
    "            score=\"sb_minilm\"\n",
    "        )\n",
    "        scores[filename]['bertscore'] = bs\n",
    "        scores[filename]['sb_mililm'] = sb_minilm\n",
    "\n",
    "    return scores\n",
    "\n",
    "def get_leak_rates_for_separate_categories(filenames, n_generations=1):\n",
    "    \"\"\"\n",
    "    Calculate leak rates for separate categories within data files using BERTScore and SBERT MiniLM.\n",
    "\n",
    "    Args:\n",
    "        filenames (list of str): List of file paths to the data files.\n",
    "        n_generations (int, optional): Number of generations to evaluate. Defaults to 1.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary containing leak rates for each file, category, and scoring method.\n",
    "    \"\"\"\n",
    "    scores_for_cat = {}\n",
    "\n",
    "    for filename in tqdm(filenames, desc=\"Processing files\"):\n",
    "        scores_for_cat[filename] = {}\n",
    "        df = pd.read_excel(filename)\n",
    "\n",
    "        categories = df['category'].dropna().unique()\n",
    "        bertscore_scores = {}\n",
    "        sb_minilm_scores = {}\n",
    "\n",
    "        for cat in categories:\n",
    "            bs_cat = get_leak_rate_for_df(\n",
    "                df,\n",
    "                n_generations=n_generations,\n",
    "                score=\"bertscore\",\n",
    "                cat=cat\n",
    "            )\n",
    "            sb_minilm_cat = get_leak_rate_for_df(\n",
    "                df,\n",
    "                n_generations=n_generations,\n",
    "                score=\"sb_minilm\",\n",
    "                cat=cat\n",
    "            )\n",
    "            bertscore_scores[str(cat)] = bs_cat\n",
    "            sb_minilm_scores[str(cat)] = sb_minilm_cat\n",
    "\n",
    "        scores_for_cat[filename]['bertscore'] = bertscore_scores\n",
    "        scores_for_cat[filename]['sb_mililm'] = sb_minilm_scores\n",
    "\n",
    "    return scores_for_cat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leak-rate for original dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_prompts/initial_prompts_qwen_05_instr.xlsx:\n",
      "bertscore = 69.27\n",
      "sb_mililm = 77.52\n",
      "initial_prompts/initial_prompts_qwen_15_instr.xlsx:\n",
      "bertscore = 75.23\n",
      "sb_mililm = 79.36\n",
      "initial_prompts/initial_prompts_qwen_3_instr.xlsx:\n",
      "bertscore = 83.03\n",
      "sb_mililm = 73.85\n",
      "initial_prompts/initial_prompts_qwen_7_instr.xlsx:\n",
      "bertscore = 71.1\n",
      "sb_mililm = 78.44\n"
     ]
    }
   ],
   "source": [
    "filenames_initial_prompts = [\n",
    "    \"initial_prompts/initial_prompts_qwen_05_instr.xlsx\",\n",
    "    \"initial_prompts/initial_prompts_qwen_15_instr.xlsx\",\n",
    "    \"initial_prompts/initial_prompts_qwen_3_instr.xlsx\",\n",
    "    \"initial_prompts/initial_prompts_qwen_7_instr.xlsx\",\n",
    "]\n",
    "\n",
    "scores = get_leak_rates_for_all_model_sizes(filenames_initial_prompts)\n",
    "\n",
    "for filename, score_info in scores.items():\n",
    "    print(f\"{filename}:\")\n",
    "\n",
    "    for metric, value in score_info.items():\n",
    "        print(f\"{metric} = {round(value * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leak-rate for color-related dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "initial_prompts/initial_prompts_qwen_05_instr.xlsx:\n",
      "bertscore = 69.27\n",
      "sb_mililm = 77.52\n",
      "initial_prompts/initial_prompts_qwen_15_instr.xlsx:\n",
      "bertscore = 75.23\n",
      "sb_mililm = 79.36\n",
      "initial_prompts/initial_prompts_qwen_3_instr.xlsx:\n",
      "bertscore = 83.03\n",
      "sb_mililm = 73.85\n",
      "initial_prompts/initial_prompts_qwen_7_instr.xlsx:\n",
      "bertscore = 71.1\n",
      "sb_mililm = 78.44\n"
     ]
    }
   ],
   "source": [
    "filenames_color_related_prompts = [\n",
    "    \"color_prompts/color_extended/color_prompts_qwen_05_instr_ext.xlsx\",\n",
    "    \"color_prompts/color_extended/color_prompts_qwen_15_instr_ext.xlsx\",\n",
    "    \"color_prompts/color_extended/color_prompts_qwen_3_instr_ext.xlsx\",\n",
    "    \"color_prompts/color_extended/color_prompts_qwen_7_instr_ext.xlsx\"\n",
    "]\n",
    "\n",
    "scores_col = get_leak_rates_for_all_model_sizes(filenames_color_related_prompts, n_generations=5)\n",
    "\n",
    "for filename, score_info in scores.items():\n",
    "    print(f\"{filename}:\")\n",
    "\n",
    "    for metric, value in score_info.items():\n",
    "        print(f\"{metric} = {round(value * 100, 2)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leak_rate for color-related dataset by category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d8d0a5f38c143a8bd2b7f73d2f624de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Processing files:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "color_prompts/color_extended/color_prompts_qwen_05_instr_ext.xlsx:\n",
      "cat 1: bertscore = 65.03\n",
      "cat 2: bertscore = 56.33\n",
      "cat 3: bertscore = 71.22\n",
      "cat 1: sb_mililm = 70.61\n",
      "cat 2: sb_mililm = 51.85\n",
      "cat 3: sb_mililm = 70.61\n",
      "color_prompts/color_extended/color_prompts_qwen_15_instr_ext.xlsx:\n",
      "cat 1: bertscore = 73.18\n",
      "cat 2: bertscore = 70.36\n",
      "cat 3: bertscore = 66.33\n",
      "cat 1: sb_mililm = 72.79\n",
      "cat 2: sb_mililm = 66.36\n",
      "cat 3: sb_mililm = 61.02\n",
      "color_prompts/color_extended/color_prompts_qwen_3_instr_ext.xlsx:\n",
      "cat 1: bertscore = 70.45\n",
      "cat 2: bertscore = 84.36\n",
      "cat 3: bertscore = 57.76\n",
      "cat 1: sb_mililm = 73.76\n",
      "cat 2: sb_mililm = 86.91\n",
      "cat 3: sb_mililm = 72.45\n",
      "color_prompts/color_extended/color_prompts_qwen_7_instr_ext.xlsx:\n",
      "cat 1: bertscore = 88.45\n",
      "cat 2: bertscore = 58.36\n",
      "cat 3: bertscore = 60.82\n",
      "cat 1: sb_mililm = 70.3\n",
      "cat 2: sb_mililm = 62.48\n",
      "cat 3: sb_mililm = 64.9\n"
     ]
    }
   ],
   "source": [
    "scores_for_cat = get_leak_rates_for_separate_categories(filenames_color_related_prompts, n_generations=5)\n",
    "\n",
    "for filename, score_info in scores_for_cat.items():\n",
    "    print(f\"{filename}:\")\n",
    "\n",
    "    for metric, cat_info in score_info.items():\n",
    "        for cat, value in cat_info.items():\n",
    "            if value:\n",
    "                print(f\"cat {cat}: {metric} = {round(value * 100, 2)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
